{"cells":[{"metadata":{},"cell_type":"markdown","source":"Hello, our goal in this notebook is to build  a model which can accurately predict if the SMS is spam or not spam.\n\nWe will be converting Text to vectors.\nWe will be using classifiers to differentiate between spam and not spam."},{"metadata":{},"cell_type":"markdown","source":"## Importing Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer ","execution_count":263,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(r'/kaggle/input/sms-spam-collection-dataset/spam.csv', encoding='latin-1')","execution_count":264,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df.head()","execution_count":265,"outputs":[{"output_type":"execute_result","execution_count":265,"data":{"text/plain":"     v1                                                 v2 Unnamed: 2  \\\n0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n1   ham                      Ok lar... Joking wif u oni...        NaN   \n2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n3   ham  U dun say so early hor... U c already then say...        NaN   \n4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n\n  Unnamed: 3 Unnamed: 4  \n0        NaN        NaN  \n1        NaN        NaN  \n2        NaN        NaN  \n3        NaN        NaN  \n4        NaN        NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>v1</th>\n      <th>v2</th>\n      <th>Unnamed: 2</th>\n      <th>Unnamed: 3</th>\n      <th>Unnamed: 4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":266,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5572 entries, 0 to 5571\nData columns (total 5 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   v1          5572 non-null   object\n 1   v2          5572 non-null   object\n 2   Unnamed: 2  50 non-null     object\n 3   Unnamed: 3  12 non-null     object\n 4   Unnamed: 4  6 non-null      object\ndtypes: object(5)\nmemory usage: 217.8+ KB\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Since we have many **NULL** values in column 3,4,5 we will be removing the columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis = 1) ","execution_count":267,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":268,"outputs":[{"output_type":"execute_result","execution_count":268,"data":{"text/plain":"     v1                                                 v2\n0   ham  Go until jurong point, crazy.. Available only ...\n1   ham                      Ok lar... Joking wif u oni...\n2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n3   ham  U dun say so early hor... U c already then say...\n4   ham  Nah I don't think he goes to usf, he lives aro...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>v1</th>\n      <th>v2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Will Check for the null value if there is any in the second columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['v2'].isnull().sum()","execution_count":269,"outputs":[{"output_type":"execute_result","execution_count":269,"data":{"text/plain":"0"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### We will rename columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.rename(columns={\"v1\": \"status\", \"v2\": \"content\"})","execution_count":270,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":271,"outputs":[{"output_type":"execute_result","execution_count":271,"data":{"text/plain":"(5572, 2)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"We need to preprocess the data. Preprocessing data usually helps in increasing accuracy of the model.\nIn preprocess we will.\n* Remove emails\n* Remove Urls\n* Remove Numbers\n* Remove non alph-numeric\n* Remove stop words if necessary"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstopWords = set(stopwords.words('english'))","execution_count":272,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def stop_words(text):\n    words = text.split(\" \")\n    # Lemmatization \n    lemmatizer = WordNetLemmatizer()\n    words_lemmatizer = [lemmatizer.lemmatize(ele) for ele in words]\n    nltk_remove_sw = [ele for ele in words_lemmatizer if ele not in stopWords]\n\n    sentence_lemmatized = \" \".join(nltk_remove_sw)\n    \n    return sentence_lemmatized","execution_count":273,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pre_process(text):\n    tx1 = re.sub('(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})', '', text) # URL's without http\n    tx2 = re.sub('[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)', '', tx1)# URL's with HTTP\n    tx3 = re.sub('[\\w\\.-]+@[\\w\\.-]+', '', tx2)  # Emails\n    tx4 = re.sub(r'[0-9]+', '', tx3) # Numbers\n    tx5 = re.sub(r\"[^A-Za-z0-9 ]\", ' ', tx4) # Non aplha-numerics\n    tx6 = re.sub(\"\\s\\s+\", \" \", tx5) \n    tx7 = tx6.lower() \n    sentence_lemmatized  = stop_words(tx7)\n    return sentence_lemmatized","execution_count":301,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pre_processed_text = []\nfor ele in df['content']:\n    pre_processed_text.append(pre_process(ele))\n    ","execution_count":275,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.insert(loc=2, column='Pre_processed', value=pre_processed_text)","execution_count":276,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":277,"outputs":[{"output_type":"execute_result","execution_count":277,"data":{"text/plain":"  status                                            content  \\\n0    ham  Go until jurong point, crazy.. Available only ...   \n1    ham                      Ok lar... Joking wif u oni...   \n2   spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n3    ham  U dun say so early hor... U c already then say...   \n4    ham  Nah I don't think he goes to usf, he lives aro...   \n\n                                       Pre_processed  \n0  go jurong point crazy available bugis n great ...  \n1                           ok lar joking wif u oni   \n2  free entry wkly comp win fa cup final tkts st ...  \n3               u dun say early hor u c already say   \n4                nah think go usf life around though  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>status</th>\n      <th>content</th>\n      <th>Pre_processed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n      <td>go jurong point crazy available bugis n great ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n      <td>ok lar joking wif u oni</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n      <td>free entry wkly comp win fa cup final tkts st ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n      <td>u dun say early hor u c already say</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n      <td>nah think go usf life around though</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":278,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5572 entries, 0 to 5571\nData columns (total 3 columns):\n #   Column         Non-Null Count  Dtype \n---  ------         --------------  ----- \n 0   status         5572 non-null   object\n 1   content        5572 non-null   object\n 2   Pre_processed  5572 non-null   object\ndtypes: object(3)\nmemory usage: 130.7+ KB\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### We will split data to training and testing\n\nFrom Data Frame the content is the input and status is the output."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df['Pre_processed'], df['status'], test_size=0.35, random_state=42)","execution_count":279,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above code we have treated the df['content'] has input and df['status'] as output. We have set the test_data to be  35% of the main dataset and random_state is 42 by convention, random_state shuffels the rows and create datframe."},{"metadata":{},"cell_type":"markdown","source":"## For System to process we need to convert **Text** to **Numbers**\nUsed to transform text to a vector of term / token counts."},{"metadata":{"trusted":true},"cell_type":"code","source":"vector = CountVectorizer(max_df=0.7)\nX_train_vector = vector.fit_transform(X_train)\nX_test_vector = vector.transform(X_test)","execution_count":294,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(X_train_vector.toarray(), columns=vector.get_feature_names())","execution_count":302,"outputs":[{"output_type":"execute_result","execution_count":302,"data":{"text/plain":"      aa  aah  aaooooright  aathi  ab  abbey  abeg  aberdeen  abi  ability  \\\n0      0    0            0      0   0      0     0         0    0        0   \n1      0    0            0      0   0      0     0         0    0        0   \n2      0    0            0      0   0      0     0         0    0        0   \n3      0    0            0      0   0      0     0         0    0        0   \n4      0    0            0      0   0      0     0         0    0        0   \n...   ..  ...          ...    ...  ..    ...   ...       ...  ...      ...   \n3616   0    0            0      0   0      0     0         0    0        0   \n3617   0    0            0      0   0      0     0         0    0        0   \n3618   0    0            0      0   0      0     0         0    0        0   \n3619   0    0            0      0   0      0     0         0    0        0   \n3620   0    0            0      0   0      0     0         0    0        0   \n\n      ...  yup  yupz  zaher  zebra  zed  zero  zf  zogtorius  zoom  zouk  \n0     ...    0     0      0      0    0     0   0          0     0     0  \n1     ...    0     0      0      0    0     0   0          0     0     0  \n2     ...    0     0      0      0    0     0   0          0     0     0  \n3     ...    0     0      0      0    0     1   0          0     0     0  \n4     ...    0     0      0      0    0     0   0          0     0     0  \n...   ...  ...   ...    ...    ...  ...   ...  ..        ...   ...   ...  \n3616  ...    0     0      0      0    0     0   0          0     0     0  \n3617  ...    0     0      0      0    0     0   0          0     0     0  \n3618  ...    0     0      0      0    0     0   0          0     0     0  \n3619  ...    0     0      0      0    0     0   0          0     0     0  \n3620  ...    0     0      0      0    0     0   0          0     0     0  \n\n[3621 rows x 5621 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>aa</th>\n      <th>aah</th>\n      <th>aaooooright</th>\n      <th>aathi</th>\n      <th>ab</th>\n      <th>abbey</th>\n      <th>abeg</th>\n      <th>aberdeen</th>\n      <th>abi</th>\n      <th>ability</th>\n      <th>...</th>\n      <th>yup</th>\n      <th>yupz</th>\n      <th>zaher</th>\n      <th>zebra</th>\n      <th>zed</th>\n      <th>zero</th>\n      <th>zf</th>\n      <th>zogtorius</th>\n      <th>zoom</th>\n      <th>zouk</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3616</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3617</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3618</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3619</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3620</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>3621 rows Ã— 5621 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## We will  check with a few classifier"},{"metadata":{},"cell_type":"markdown","source":"## Gaussian Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nclf_GNB = GaussianNB()\nclf_GNB.fit(X_train_vector.toarray(), y_train)\nprint(f'accuracy: {clf_GNB.score(X_test_vector.toarray(), y_test )}')","execution_count":303,"outputs":[{"output_type":"stream","text":"accuracy: 0.8774987186058432\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nclf_DTC = DecisionTreeClassifier()\nclf_DTC.fit(X_train_vector, y_train)\nprint(f'accuracy: {clf_DTC.score(X_test_vector, y_test)}')","execution_count":304,"outputs":[{"output_type":"stream","text":"accuracy: 0.9728344438749359\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nclf_SVC = SVC()\nclf_SVC.fit(X_train_vector, y_train)\nprint(f'accuracy: {clf_SVC.score(X_test_vector, y_test)}')","execution_count":305,"outputs":[{"output_type":"stream","text":"accuracy: 0.97385955920041\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclf_LR = LogisticRegression()\nclf_LR.fit(X_train_vector, y_train)\nprint(f'accuracy: {clf_LR.score(X_test_vector, y_test)}')","execution_count":307,"outputs":[{"output_type":"stream","text":"accuracy: 0.9805228088159919\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Summary\n\n* We started with checking the NULL values, and droped few columns\n* We did few pre-processing test\n    * This included removal of emails, Urls, Numbers etc...    \n* We then converted text to a vector\n* We applied this vectors to different classifier, and found accuracy of the model. We performed this on the test data set.\n\n### Observation\n* Even without pre-processing the text, their was no big difference in the model accuracy.\n* Their was a slight increase in model accuracy with **max_df** set to 0.7 in CountVectorizer\n* Tried the model with bigram, accuracy went down a bit on few models. No major improvements\n"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}